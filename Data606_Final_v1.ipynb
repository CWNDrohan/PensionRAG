{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CWNDrohan/PensionRAG/blob/main/Data606_Final_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 ‚Äî Install Libraries & Imports"
      ],
      "metadata": {
        "id": "nEleQHQQA0XQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdec1lmzAtt-"
      },
      "outputs": [],
      "source": [
        "# üõ†Ô∏è Install Required Libraries (FAISS + LlamaIndex only)\n",
        "!pip install -q \\\n",
        "    llama-index \\\n",
        "    llama-index-vector-stores-faiss \\\n",
        "    llama-index-embeddings-huggingface \\\n",
        "    llama-index-llms-huggingface \\\n",
        "    sentence-transformers transformers \\\n",
        "    pdfplumber PyMuPDF \\\n",
        "    faiss-cpu\n",
        "\n",
        "# üì• Step 1b: Import All Necessary Libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import faiss\n",
        "import shutil\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "import re\n",
        "import pprint  # üîç Pretty-printing for debug visibility\n",
        "import pandas as pd\n",
        "from google.colab import drive, userdata\n",
        "from datetime import datetime\n",
        "\n",
        "# ‚úÖ Transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ‚úÖ LlamaIndex Imports\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, Document\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
        "\n",
        "# üìè Global Chunking Configuration\n",
        "CHUNK_SIZE = 384\n",
        "CHUNK_OVERLAP = 96\n",
        "\n",
        "# ‚úÖ Centralized keyword list\n",
        "KEYWORDS = [\n",
        "    \"pension formula\",\n",
        "    \"early retirement\",\n",
        "    \"benefit reduction\",\n",
        "    \"final average salary\",\n",
        "    \"penalty table\"\n",
        "]\n",
        "\n",
        "# üîó Step 1c: Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 ‚Äî Verify Documents & Tokens are Accessible"
      ],
      "metadata": {
        "id": "5CfHOp42A6Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÇ Define Paths and Load JSON Knowledge Base\n",
        "\n",
        "# üìÑ Define permanent location\n",
        "drive_dir = \"/content/drive/My Drive/School/UMBC/DATA606/Input\"\n",
        "kb_filename = \"knowledge_base.json\"\n",
        "pension_pdf_path = os.path.join(drive_dir, \"NYCERS_Tier6.pdf\")\n",
        "\n",
        "# üß† Wrapped loader\n",
        "def load_latest_kb(temp_dir=\"/content\", drive_dir=drive_dir, filename=kb_filename):\n",
        "    # üîç Auto-locate temp file\n",
        "    temp_json = None\n",
        "    for file in os.listdir(temp_dir):\n",
        "        if file.endswith(\".json\"):\n",
        "            temp_json = os.path.join(temp_dir, file)\n",
        "            print(f\"üìÇ Found new JSON in temp directory: {file}\")  # ‚úÖ NEW LINE\n",
        "            break\n",
        "\n",
        "    drive_json_path = os.path.join(drive_dir, filename)\n",
        "\n",
        "    # üîÑ Copy and rename\n",
        "    if temp_json:\n",
        "        shutil.copy(temp_json, drive_json_path)\n",
        "        mod_time = os.path.getmtime(drive_json_path)\n",
        "        timestamp = datetime.fromtimestamp(mod_time).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"‚úÖ Knowledge base copied to Drive as: {filename}\")\n",
        "        print(f\"üïí Last modified: {timestamp}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No new JSON found in /content. Using existing file in Drive.\")\n",
        "\n",
        "    # üìñ Load\n",
        "    if os.path.exists(drive_json_path):\n",
        "        with open(drive_json_path, \"r\") as f:\n",
        "            kb = json.load(f)\n",
        "            print(\"‚úÖ Knowledge base loaded with keys:\", list(kb.keys()))\n",
        "            return kb\n",
        "    else:\n",
        "        raise FileNotFoundError(\"‚ùå ERROR: No knowledge_base.json found in Drive!\")\n",
        "\n",
        "# ‚úÖ Load KB\n",
        "knowledge_base = load_latest_kb()\n",
        "\n",
        "# üìÑ Validate PDF path\n",
        "if os.path.exists(pension_pdf_path):\n",
        "    print(\"‚úÖ Pension PDF is accessible:\", pension_pdf_path)\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Pension PDF not found! Check the file path.\")\n",
        "\n",
        "# üîê Retrieve Hugging Face token from Colab's secrets\n",
        "huggingface_token = userdata.get(\"HF_TOKEN\")\n",
        "if huggingface_token:\n",
        "    print(\"‚úÖ Hugging Face token retrieved successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Hugging Face token not found! Make sure it's saved in Colab.\")"
      ],
      "metadata": {
        "id": "i6O6fLoFBMTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 -- Extract, Clean, Tag, and Index Pension Text and Tables for RAG Processing"
      ],
      "metadata": {
        "id": "ULHsR0sbj52D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Extract, Clean, and Tag Pension Text + Tables\n",
        "\n",
        "# üßº Clean raw text (remove headers, collapse whitespace, etc.)\n",
        "def clean_raw_text(text):\n",
        "    text = re.sub(r'\\n?\\d{1,3}\\nSummary Plan Description[^\\n]*', '', text)  # Remove headers\n",
        "    text = re.sub(r'\\n{2,}', '\\n\\n', text)                                  # Collapse newlines\n",
        "    text = re.sub(r'\\n\\s+\\n', '\\n\\n', text)                                 # Remove whitespace-only lines\n",
        "    return text.strip()\n",
        "\n",
        "# üìÑ Extract full text from PDF (fitz for layout)\n",
        "def extract_raw_text_from_pdf(pdf_path):\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        return \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "\n",
        "# üìä Extract tables from PDF (pdfplumber is best for tables)\n",
        "def extract_tables_from_pdf(pdf_path):\n",
        "    extracted_tables = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            for table in page.extract_tables():\n",
        "                df = pd.DataFrame(table)\n",
        "                extracted_tables.append(df.to_dict(orient=\"records\"))\n",
        "    return extracted_tables\n",
        "\n",
        "# üß† Optional metadata tagging for known table types\n",
        "def tag_table_metadata(table_data, table_id):\n",
        "    table_text = str(table_data)\n",
        "\n",
        "    # ‚úÖ Base keywords via ID-specific tagging\n",
        "    if table_id in [5, 6]:\n",
        "        manual_keywords = [\"pension formula\", \"final average salary\", \"35% FAS\", \"2% additional\"]\n",
        "        nl_summary = (\n",
        "            \"This table describes the pension formula:\\n\"\n",
        "            \"- If you have less than 20 years of service: 1.67% √ó Final Average Salary √ó Years of Service.\\n\"\n",
        "            \"- If you have 20 or more years: 35% of FAS for the first 20 years, plus 2% for each year beyond 20.\\n\"\n",
        "        )\n",
        "        table_text = nl_summary + \"\\n\" + table_text\n",
        "    elif table_id == 7:\n",
        "        manual_keywords = [\"early retirement\", \"age reduction\", \"6.5%\", \"penalty table\"]\n",
        "    else:\n",
        "        manual_keywords = []\n",
        "\n",
        "    # ‚úÖ Dynamically match keywords from table content\n",
        "    dynamic_keywords = [kw for kw in KEYWORDS if kw in table_text.lower()]\n",
        "\n",
        "    # ‚úÖ Combine both sets (deduplicated)\n",
        "    all_keywords = sorted(set(manual_keywords + dynamic_keywords))\n",
        "\n",
        "    return Document(\n",
        "        text=table_text,\n",
        "        metadata={\n",
        "            \"table_id\": table_id,\n",
        "            \"source\": f\"table_{table_id}\",\n",
        "            \"table_keywords\": all_keywords\n",
        "        }\n",
        "    )\n",
        "\n",
        "# üèóÔ∏è Run pipeline\n",
        "raw_text = clean_raw_text(extract_raw_text_from_pdf(pension_pdf_path))\n",
        "extracted_tables = extract_tables_from_pdf(pension_pdf_path)\n",
        "\n",
        "# üì¶ Wrap raw text and tagged tables into Document objects\n",
        "combined_docs = [Document(text=raw_text, metadata={\"type\": \"full_text\"})]\n",
        "combined_docs += [tag_table_metadata(tbl, i) for i, tbl in enumerate(extracted_tables)]\n",
        "\n",
        "print(f\"‚úÖ Extracted raw text ({len(raw_text):,} characters)\")\n",
        "print(f\"‚úÖ Extracted {len(extracted_tables)} tables and tagged key pension tables.\")"
      ],
      "metadata": {
        "id": "qvJj1hYIj6_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 ‚Äî Build & Verify the Base Index"
      ],
      "metadata": {
        "id": "FIXYkEMmtnMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Chunk Pension Text + Tables and Build FAISS Vector Index\n",
        "\n",
        "# ‚úÖ Define paths\n",
        "faiss_index_path = \"/content/faiss_index\"\n",
        "\n",
        "# üßº Remove old FAISS index (if it exists)\n",
        "if os.path.exists(faiss_index_path):\n",
        "    shutil.rmtree(faiss_index_path)\n",
        "print(\"üßº Old FAISS index removed.\")\n",
        "\n",
        "# üîç Optional: Measure total character length of all input documents\n",
        "total_chars = sum(len(doc.text) for doc in combined_docs)\n",
        "print(f\"üîç Total combined length across all documents: {total_chars:,} characters\")\n",
        "\n",
        "# ‚úÖ Use sentence-based chunking for precise control\n",
        "splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "nodes = splitter.get_nodes_from_documents(combined_docs)\n",
        "print(f\"üì¶ Chunking complete: {len(nodes)} chunks created\")\n",
        "\n",
        "# üîç Preview a few chunks\n",
        "print(\"\\nüîç Sample of Chunked Nodes:\\n\")\n",
        "for i, node in enumerate(nodes[:3]):\n",
        "    print(f\"üîπ Chunk {i+1}\")\n",
        "    pprint.pprint(node.metadata)\n",
        "    print(node.text[:500])\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "# ‚úÖ Define FAISS index and vector store\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "faiss_index = faiss.IndexFlatL2(768)\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# ‚úÖ Build and persist the vector index\n",
        "pension_index = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "pension_index.storage_context.persist(persist_dir=faiss_index_path)\n",
        "print(\"‚úÖ FAISS index with sentence-based chunking saved.\")\n",
        "\n",
        "# üîç Confirm FAISS index structure\n",
        "print(\"\\nüìö FAISS Index Sample Preview:\\n\")\n",
        "retriever = pension_index.as_retriever(similarity_top_k=3)\n",
        "sample_query = \"What is the pension formula?\"\n",
        "retrieved = retriever.retrieve(sample_query)\n",
        "\n",
        "for i, node in enumerate(retrieved):\n",
        "    print(f\"üîπ Result {i+1}\")\n",
        "    pprint.pprint(node.metadata)\n",
        "    print(node.text[:800])\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "_mAuMk-pBaEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 ‚Äî Load and Configure LLM with Query Engine"
      ],
      "metadata": {
        "id": "27mN0tWRBXIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Load Mistral-7B-Instruct-v0.1 model using HuggingFaceLLM\n",
        "\n",
        "# üîÅ Swap in new model version\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# üß† Load tokenizer to get eos_token_id\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ‚úÖ Configure LLM with optimized generation parameters\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=3900,\n",
        "    max_new_tokens=512,\n",
        "    generate_kwargs={\n",
        "        \"do_sample\": False,         # Deterministic response\n",
        "        \"temperature\": 0.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 50,\n",
        "        \"pad_token_id\": eos_token_id\n",
        "    },\n",
        "    tokenizer_name=model_id,\n",
        "    model_name=model_id,\n",
        "    device_map=\"auto\",\n",
        "    tokenizer_kwargs={\"use_fast\": True},\n",
        "    model_kwargs={\"torch_dtype\": \"auto\"}  # Use float16 if memory is tight\n",
        ")\n",
        "\n",
        "# ‚úÖ Reinitialize query engine with updated LLM\n",
        "query_engine = pension_index.as_query_engine(llm=llm)\n",
        "\n",
        "print(\"‚úÖ Mistral-7B-Instruct-v0.1 loaded successfully and ready to go!\")"
      ],
      "metadata": {
        "id": "-cjK2tMfY5ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6 -- Add Metadata-Aware Reranking Function"
      ],
      "metadata": {
        "id": "NHvLp7jUbVpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Generalized Metadata-Aware Reranking Function\n",
        "def rerank_with_metadata_priority(nodes, keyword_weights=None):\n",
        "    \"\"\"\n",
        "    Boosts scores of nodes that match specified metadata keywords.\n",
        "\n",
        "    Args:\n",
        "        nodes (List[NodeWithScore]): Retrieved nodes from query_engine.\n",
        "        keyword_weights (dict): Keys = keywords or phrases, values = boost weights.\n",
        "\n",
        "    Returns:\n",
        "        List[NodeWithScore]: Re-ranked nodes sorted by boosted score.\n",
        "    \"\"\"\n",
        "    if keyword_weights is None:\n",
        "        keyword_weights = {kw: 0.2 for kw in KEYWORDS}\n",
        "        keyword_weights[\"pension formula\"] = 0.3  # Slightly boost the most important one\n",
        "\n",
        "    reranked = []\n",
        "    for node in nodes:\n",
        "        base_score = node.score or 0\n",
        "        metadata = node.metadata or {}\n",
        "        keywords = metadata.get(\"table_keywords\", [])\n",
        "        boost = sum(weight for kw, weight in keyword_weights.items() if any(kw in k.lower() for k in keywords))\n",
        "        node.score = base_score + boost\n",
        "        reranked.append(node)\n",
        "\n",
        "    return sorted(reranked, key=lambda x: x.score, reverse=True)"
      ],
      "metadata": {
        "id": "4Lj3C75_N7_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7 -- Generalized retrieval + metadata injection + reranking"
      ],
      "metadata": {
        "id": "BtMQYaRlMekG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Generalized retrieval + metadata injection + reranking\n",
        "def get_reranked_nodes(query, index, reranker_fn, keyword_list=KEYWORDS):\n",
        "    from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "    # Step 1: Vector-based retrieval\n",
        "    retrieved_nodes = query_engine.retrieve(query)\n",
        "\n",
        "    # Step 2: Inject nodes with matching metadata keywords\n",
        "    extra_nodes = []\n",
        "    for node in index.docstore.docs.values():\n",
        "        metadata = node.metadata or {}\n",
        "        keywords = metadata.get(\"table_keywords\", [])\n",
        "        if any(kw.lower() in keyword_list for kw in keywords):\n",
        "            extra_nodes.append(NodeWithScore(node=node, score=0.0))\n",
        "\n",
        "    # Step 3: Combine both sets, deduplicating\n",
        "    combined_node_ids = {n.node.node_id if isinstance(n, NodeWithScore) else n.node_id for n in retrieved_nodes}\n",
        "    for extra in extra_nodes:\n",
        "        if extra.node.node_id not in combined_node_ids:\n",
        "            retrieved_nodes.append(extra)\n",
        "\n",
        "    # Step 4: Rerank using provided reranker\n",
        "    return reranker_fn(retrieved_nodes)"
      ],
      "metadata": {
        "id": "qSc9KAkgvJLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8 -- Extract Inputs from natural language query"
      ],
      "metadata": {
        "id": "PpNNymhCnCts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Extract inputs from natural language user query\n",
        "\n",
        "def extract_inputs(user_query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract retirement_age, years_of_service, final_salary, and list of salaries\n",
        "    from a natural language pension query using regex-based parsing.\n",
        "    \"\"\"\n",
        "    # Match $100,000 style\n",
        "    dollar_matches = re.findall(r\"\\$\\s*([\\d,]{3,})\", user_query)\n",
        "    dollar_salaries = [int(s.replace(\",\", \"\")) for s in dollar_matches]\n",
        "    print(f\"üí∞ Found dollar-style salaries: {dollar_salaries}\")\n",
        "\n",
        "    # Match $110k style\n",
        "    dollar_k_matches = re.findall(r\"\\$\\s*(\\d{2,3})\\s*[kK]\\b\", user_query)\n",
        "    dollar_k_salaries = [int(k) * 1000 for k in dollar_k_matches]\n",
        "    print(f\"üí∞ Found dollar-k style salaries: {dollar_k_salaries}\")\n",
        "\n",
        "    # Match 110k style (no $)\n",
        "    plain_k_matches = re.findall(r\"(?<!\\$)(?<!\\d)(\\d{2,3})\\s*[kK]\\b\", user_query)\n",
        "    plain_k_salaries = [int(k) * 1000 for k in plain_k_matches]\n",
        "    print(f\"üí∞ Found plain-k style salaries: {plain_k_salaries}\")\n",
        "\n",
        "    # Combine all salaries and filter\n",
        "    all_salaries = sorted(set(s for s in (\n",
        "        dollar_salaries + dollar_k_salaries + plain_k_salaries\n",
        "    ) if s >= 1000))\n",
        "    print(f\"üí∞ Combined salary list (filtered): {all_salaries}\")\n",
        "\n",
        "    final_salary = all_salaries[-1] if all_salaries else None\n",
        "    final_salary_list = all_salaries if len(all_salaries) > 1 else None\n",
        "\n",
        "    # Extract current age\n",
        "    age_match = re.search(r\"(?:I['‚Äô]m|I am|I'm)\\s*(\\d{2})\", user_query)\n",
        "    current_age = int(age_match.group(1)) if age_match else None\n",
        "    print(f\"üéÇ Parsed current age: {current_age}\")\n",
        "\n",
        "    # Extract retirement age\n",
        "    ret_match = re.search(r\"(?:retir(?:e|ing|ement)[^\\d]{0,10}|at age\\s*)(\\d{2})\", user_query)\n",
        "    retirement_age = int(ret_match.group(1)) if ret_match else current_age\n",
        "    print(f\"üéØ Parsed retirement age: {retirement_age}\")\n",
        "\n",
        "    # Extract current years of service\n",
        "    yos_match = re.search(r\"(?:worked|been here|employed).{0,20}?(\\d{1,2})\\s*(?:years|yrs)\", user_query)\n",
        "    current_yos = int(yos_match.group(1)) if yos_match else None\n",
        "    print(f\"üõ†Ô∏è Parsed current YOS: {current_yos}\")\n",
        "\n",
        "    # Alternative YOS\n",
        "    yos_alt_match = re.search(\n",
        "        r\"(?:after|for|with|total of)?\\s*(\\d{2})\\s*(?:years|yrs)(?!\\s*old)\",\n",
        "        user_query, re.IGNORECASE\n",
        "    )\n",
        "    yos_alt = int(yos_alt_match.group(1)) if yos_alt_match else None\n",
        "    print(f\"üõ†Ô∏è Parsed alternative YOS: {yos_alt}\")\n",
        "\n",
        "    # Final YOS computation\n",
        "    if current_yos and not yos_alt:\n",
        "        if current_age and retirement_age:\n",
        "            years_of_service = (retirement_age - current_age) + current_yos\n",
        "        else:\n",
        "            years_of_service = current_yos\n",
        "    else:\n",
        "        years_of_service = yos_alt or current_yos or None\n",
        "\n",
        "    print(f\"‚úÖ Final computed Years of Service: {years_of_service}\")\n",
        "    print(f\"‚úÖ Final Salary for calculation: {final_salary}\")\n",
        "\n",
        "    # Flag whether retirement is before age 63\n",
        "    penalty_flag = False\n",
        "    if retirement_age and retirement_age < 63:\n",
        "        penalty_flag = True\n",
        "\n",
        "    # Include in return object\n",
        "    return {\n",
        "        \"retirement_age\": retirement_age,\n",
        "        \"years_of_service\": years_of_service,\n",
        "        \"final_salary\": final_salary,\n",
        "        \"salaries\": final_salary_list,\n",
        "        \"penalty_flag\": penalty_flag\n",
        "    }\n",
        "\n",
        "# üîç Run extraction with test user query\n",
        "user_query = \"I‚Äôm Kevin, I‚Äôm 56, I‚Äôve worked here for 30 years, and I make $100,000.\"\n",
        "inputs = extract_inputs(user_query)\n",
        "\n",
        "print(\"\\nüéØ FINAL PARSED INPUTS (to be used in model query and math):\")\n",
        "for key, val in inputs.items():\n",
        "    print(f\"{key}: {val}\")"
      ],
      "metadata": {
        "id": "-SzzRwI1nH1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9 -- No hard coded flag"
      ],
      "metadata": {
        "id": "0boneuuiYVit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 9: Build question with penalty clarification for early retirement\n",
        "ret_age = inputs[\"retirement_age\"]\n",
        "yos = inputs[\"years_of_service\"]\n",
        "\n",
        "if ret_age and ret_age < 63:\n",
        "    question = (\n",
        "        f\"What is the pension formula for someone with {yos} years of service? \"\n",
        "        f\"If they retire at age {ret_age}, which is before the normal retirement age of 63, \"\n",
        "        f\"is there a penalty? If so, use the penalty table listed in the documentation and state the \"\n",
        "        f\"exact penalty percentage associated with age {ret_age}. \"\n",
        "        f\"‚ö†Ô∏è Do not guess, multiply, or estimate ‚Äî quote the percentage directly from the table.\"\n",
        "    )\n",
        "else:\n",
        "    question = (\n",
        "        f\"What is the pension formula for someone with {yos} years of service? \"\n",
        "        f\"Is there a penalty if they retire at age {ret_age}?\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nüß† Dynamic LLM Query:\\n{question}\")\n"
      ],
      "metadata": {
        "id": "WCMYV26oXxvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9a -- Raw Vector Retrieval Results"
      ],
      "metadata": {
        "id": "ssx9ZcFkHojG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 9a: Print raw vector retrieval results before reranking\n",
        "\n",
        "# Use same question generated in Step 9\n",
        "raw_nodes = query_engine.retrieve(question)\n",
        "\n",
        "print(\"\\nüîé Raw Vector Retrieval Results (Pre-Reranking):\\n\")\n",
        "for i, node in enumerate(raw_nodes[:5]):\n",
        "    print(f\"üîπ Raw Rank {i+1}\")\n",
        "    print(\"üìÑ Metadata:\", node.metadata)\n",
        "    print(\"üîé Text Preview:\\n\", node.text[:800])\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "f3oOxR_eHu24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10 -- Combine Reranked Chunks into Final LLM Context"
      ],
      "metadata": {
        "id": "vNEWxegfoTR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Combine top reranked chunks into a single context block\n",
        "\n",
        "# üîπ Retrieve + Rerank using defined logic\n",
        "reranked_nodes = get_reranked_nodes(question, pension_index, rerank_with_metadata_priority)\n",
        "\n",
        "max_chunks = 5  # You can adjust this if needed\n",
        "context_blocks = [node.text.strip() for node in reranked_nodes[:max_chunks]]\n",
        "\n",
        "# üîç Detect if penalty table was included\n",
        "contains_penalty_table = any(\n",
        "    \"penalty table\" in (node.metadata.get(\"table_keywords\") or [])\n",
        "    for node in reranked_nodes[:max_chunks]\n",
        ")\n",
        "\n",
        "# üè∑Ô∏è Add a context flag if relevant\n",
        "penalty_flag = (\n",
        "    \"\\n\\n‚ö†Ô∏è Penalty table detected in context. Use it to apply early retirement reductions if age < 63.\\n\"\n",
        "    if contains_penalty_table else \"\"\n",
        ")\n",
        "\n",
        "combined_context = \"\\n\\n\".join(context_blocks) + penalty_flag\n",
        "\n",
        "print(\"‚úÖ Combined context ready (length:\", len(combined_context), \"characters)\\n\")\n",
        "\n",
        "# üß† Show which chunks were selected\n",
        "print(\"üîç Top Reranked Chunks Used in Context:\\n\")\n",
        "for i, node in enumerate(reranked_nodes[:max_chunks]):\n",
        "    print(f\"üîπ Rank {i+1} | Score: {node.score:.4f}\")\n",
        "    print(\"üìÑ Metadata:\", node.metadata)\n",
        "    print(\"üîé Text Preview:\\n\", node.text[:1000])\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "q-rjCLBUoZ3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11 -- Construct the Final Prompt"
      ],
      "metadata": {
        "id": "8irgoz-YpX27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Construct the final prompt using a dedicated system instruction block\n",
        "\n",
        "system_prompt = \"\"\"You are an expert pension plan assistant.\n",
        "\n",
        "Use only the provided context to answer questions. Do not guess or make assumptions.\n",
        "\n",
        "If the context includes a penalty table, apply it based on the retirement age.\n",
        "Only say \"no penalty\" if the table or context explicitly says so.\n",
        "\"\"\"\n",
        "\n",
        "prompt_v10 = f\"\"\"{system_prompt}\n",
        "\n",
        "Based on the documentation below, answer the question in clear, concise terms.\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"\n",
        "{combined_context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Debug: Preview the prompt going to the LLM\n",
        "print(\"\\nüìù Final Prompt Sent to LLM:\\n\", prompt_v10[:1000])  # Truncated to 1000 chars for readability"
      ],
      "metadata": {
        "id": "uEaLC1IJpbF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 12 -- Run the Model on the Prompt"
      ],
      "metadata": {
        "id": "JVG7WUslpdvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Get the model's response\n",
        "\n",
        "response = llm.complete(prompt_v10)\n",
        "response = response.text  # ‚úÖ Convert to string before parsing!\n",
        "print(\"\\nüí¨ LLM Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "SJ5Km3idphBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 13 -- Parse Model Output and Print Python-Ready Values"
      ],
      "metadata": {
        "id": "g--Orul_rtXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 13: Parse LLM response and extract values for Python pension math\n",
        "\n",
        "def parse_llm_response(text: str, fallback_inputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Parses model output to extract:\n",
        "    - base_rate (as decimal, e.g., 0.35 or 0.0167)\n",
        "    - bonus_rate (e.g., 0.02 if mentioned)\n",
        "    - bonus_years (derived from YOS - threshold)\n",
        "    - penalty_rate (as decimal, 0.0 if no penalty)\n",
        "    \"\"\"\n",
        "    # Extract numeric values directly from model output\n",
        "    base_match = re.search(r\"(\\d{1,2}(?:\\.\\d+)?)%\\s+of.*?(?:first\\s+)?(\\d{1,2})\", text.lower())\n",
        "    bonus_match = re.search(r\"(\\d{1,2}(?:\\.\\d+)?)%\\s+for each year.*?(excess|beyond)\", text.lower())\n",
        "\n",
        "    base_rate = float(base_match.group(1)) / 100 if base_match else None\n",
        "    threshold_years = int(base_match.group(2)) if base_match else 20  # fallback to 20\n",
        "\n",
        "    # Fallback: look for simple \"1.67% √ó FAS √ó YOS\" style rule\n",
        "    if not base_rate:\n",
        "        alt_match = re.search(r\"(\\d{1,2}(?:\\.\\d+)?)\\s*(?:%|percent).*?(final average salary|FAS)\", text.lower())\n",
        "        if alt_match:\n",
        "            base_rate = float(alt_match.group(1)) / 100\n",
        "            threshold_years = 0\n",
        "            print(f\"‚úÖ Fallback base_rate parsed: {base_rate:.4f}\")\n",
        "\n",
        "    bonus_rate = float(bonus_match.group(1)) / 100 if bonus_match else None\n",
        "\n",
        "    yos = fallback_inputs.get(\"years_of_service\")\n",
        "    bonus_years = yos - threshold_years if yos and yos > threshold_years else 0\n",
        "\n",
        "    # Penalty rate extraction ‚Äî safer fallback logic\n",
        "    penalty_rate = 0.0\n",
        "    if \"no penalty\" in text.lower():\n",
        "        penalty_rate = 0.0\n",
        "    else:\n",
        "        # Try both phrasing styles\n",
        "        match1 = re.search(r\"(\\d{1,2}(?:\\.\\d+)?)%\\s*(reduction|penalty)\", text.lower())\n",
        "        match2 = re.search(r\"(reduction|penalty).*?(\\d{1,2}(?:\\.\\d+)?)%\", text.lower())\n",
        "\n",
        "        if match1:\n",
        "            penalty_str = match1.group(1)\n",
        "        elif match2:\n",
        "            penalty_str = match2.group(2)\n",
        "        else:\n",
        "            penalty_str = None\n",
        "\n",
        "        if penalty_str:\n",
        "            try:\n",
        "                penalty_rate = float(penalty_str) / 100\n",
        "            except ValueError:\n",
        "                print(f\"‚ùå Failed to convert penalty string: {penalty_str}\")\n",
        "                penalty_rate = 0.0\n",
        "\n",
        "    final_salary = fallback_inputs.get(\"final_salary\")\n",
        "\n",
        "    # Print debug values\n",
        "    print(\"\\nüì§ Variables passed to Python pension calculator:\")\n",
        "    print(f\" - Final Salary: ${final_salary:,}\" if final_salary else \" - Final Salary: ‚ùå MISSING\")\n",
        "    print(f\" - Years of Service: {yos} years\" if yos else \" - Years of Service: ‚ùå MISSING\")\n",
        "    print(f\" - Retirement Age: {fallback_inputs.get('retirement_age')}\" if fallback_inputs.get(\"retirement_age\") else \" - Retirement Age: ‚ùå MISSING\")\n",
        "    print(f\" - Base Rate: {base_rate * 100:.2f}%\" if base_rate is not None else \" - Base Rate: ‚ùå MISSING\")\n",
        "    print(f\" - Bonus Rate: {bonus_rate * 100:.2f}%\" if bonus_rate is not None else \" - Bonus Rate: ‚ùå MISSING\")\n",
        "    print(f\" - Bonus Years: {bonus_years}\")\n",
        "    print(f\" - Penalty Rate: {penalty_rate * 100:.2f}%\")\n",
        "\n",
        "    # ‚úÖ DEBUG: Show raw LLM response\n",
        "    print(\"\\nüß™ Raw LLM Response Used for Parsing:\\n\", text)\n",
        "\n",
        "    return {\n",
        "        \"final_salary\": final_salary,\n",
        "        \"years_of_service\": yos,\n",
        "        \"retirement_age\": fallback_inputs.get(\"retirement_age\"),\n",
        "        \"base_rate\": base_rate,\n",
        "        \"bonus_rate\": bonus_rate,\n",
        "        \"bonus_years\": bonus_years,\n",
        "        \"penalty_rate\": penalty_rate,\n",
        "    }\n",
        "\n",
        "# üöÄ Run parser on model output (make sure this comes after response is generated in Step 12)\n",
        "parsed_vars = parse_llm_response(response, inputs)"
      ],
      "metadata": {
        "id": "-lz8AuZJrxe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 14 -- Final Python Pension Calculation"
      ],
      "metadata": {
        "id": "Ibdpf-jv48A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Calculate pension using parsed variables\n",
        "\n",
        "def calculate_final_pension(vars: dict):\n",
        "    \"\"\"\n",
        "    Calculate the pension using structured values:\n",
        "    Pension = (Base Portion + Bonus Portion) √ó (1 - Penalty)\n",
        "    \"\"\"\n",
        "    if not all([vars[\"final_salary\"], vars[\"base_rate\"], vars[\"years_of_service\"]]):\n",
        "        print(\"‚ùå Missing required inputs for pension calculation.\")\n",
        "        return\n",
        "\n",
        "    salary = vars[\"final_salary\"]\n",
        "    base_rate = vars[\"base_rate\"]\n",
        "    bonus_rate = vars.get(\"bonus_rate\", 0.0)\n",
        "    bonus_years = vars.get(\"bonus_years\", 0)\n",
        "    penalty = vars.get(\"penalty_rate\", 0.0)\n",
        "    yos = vars.get(\"years_of_service\")\n",
        "    age = vars.get(\"retirement_age\")\n",
        "\n",
        "    base_portion = base_rate * salary\n",
        "    bonus_portion = bonus_rate * salary * bonus_years\n",
        "    subtotal = base_portion + bonus_portion\n",
        "    adjusted = subtotal * (1 - penalty)\n",
        "\n",
        "    penalty_pct = penalty * 100\n",
        "    penalty_label = f\"{penalty_pct:.2f}%\" if penalty_pct > 0 else \"0.00%\"\n",
        "\n",
        "    print(\"\\nüßæ Pension Calculation Details:\")\n",
        "    print(f\" - Final Salary: ${salary:,.2f}\")\n",
        "    print(f\" - Years of Service: {yos}\")\n",
        "    print(f\" - Retirement Age: {age}\")\n",
        "    print(f\" - Base Portion: ${base_portion:,.2f}\")\n",
        "    print(f\" - Bonus Portion: ${bonus_portion:,.2f}\")\n",
        "    print(f\" - Subtotal Before Penalty: ${subtotal:,.2f}\")\n",
        "    print(f\" - Penalty Applied: {penalty_label}\")\n",
        "    print(f\"\\nüíµ Final Adjusted Pension: ${adjusted:,.2f}\")\n",
        "\n",
        "    # ‚úÖ Commented out to suppress trailing output\n",
        "    # return adjusted\n",
        "\n",
        "# üöÄ Run it\n",
        "calculate_final_pension(parsed_vars)"
      ],
      "metadata": {
        "id": "9_TP56Em4_m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 15 -- Q&A Query"
      ],
      "metadata": {
        "id": "0q-5_F9gV2Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Q&A query (non-pension calculation)\n",
        "qa_query = \"What is the military service rule?\"  # üîÅ Replace with any question\n",
        "\n",
        "# Retrieve relevant chunks\n",
        "qa_nodes = query_engine.retrieve(qa_query)\n",
        "\n",
        "# Combine top chunks into context\n",
        "qa_context = \"\\n\\n\".join([n.text.strip() for n in qa_nodes[:3]])  # Use top 3 for brevity\n",
        "\n",
        "# Construct Q&A prompt\n",
        "qa_prompt = f\"\"\"You are a pension plan assistant. Use the information below to answer the user's question clearly.\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"\n",
        "{qa_context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: {qa_query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Run the model\n",
        "qa_response = llm.complete(qa_prompt)\n",
        "\n",
        "# Print output\n",
        "print(\"\\nüí¨ Q&A Response:\\n\")\n",
        "print(qa_response)"
      ],
      "metadata": {
        "id": "sR30CFcsV54d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 16 -- Once click **rerun**"
      ],
      "metadata": {
        "id": "aKZBDl7SHHlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÅ Update with your test query here\n",
        "user_query = \"I'm Josh, I'm 60, I've worked here for 28 years, and I make $110,000.\"\n",
        "\n",
        "# Step 8: Extract structured inputs\n",
        "inputs = extract_inputs(user_query)\n",
        "\n",
        "# Step 9: Construct question based on extracted inputs\n",
        "ret_age = inputs[\"retirement_age\"]\n",
        "yos = inputs[\"years_of_service\"]\n",
        "\n",
        "if ret_age and ret_age < 63:\n",
        "    question = (\n",
        "        f\"What is the pension formula for someone with {yos} years of service? \"\n",
        "        f\"If they retire at age {ret_age}, which is before the normal retirement age of 63, \"\n",
        "        f\"is there a penalty? If so, what is the exact penalty listed in the table for age {ret_age}? \"\n",
        "        f\"Do not estimate it ‚Äî use the table if available.\"\n",
        "    )\n",
        "else:\n",
        "    question = (\n",
        "        f\"What is the pension formula for someone with {yos} years of service? \"\n",
        "        f\"Is there a penalty if they retire at age {ret_age}?\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nüß† Dynamic LLM Query:\\n{question}\")\n",
        "\n",
        "# Step 10: Retrieve and rerank chunks\n",
        "reranked_nodes = get_reranked_nodes(question, pension_index, rerank_with_metadata_priority)\n",
        "max_chunks = 5\n",
        "context_blocks = [node.text.strip() for node in reranked_nodes[:max_chunks]]\n",
        "contains_penalty_table = any(\n",
        "    \"penalty table\" in (node.metadata.get(\"table_keywords\") or [])\n",
        "    for node in reranked_nodes[:max_chunks]\n",
        ")\n",
        "penalty_flag_note = (\n",
        "    \"\\n\\n‚ö†Ô∏è Penalty table detected in context. Use it to apply early retirement reductions if age < 63.\\n\"\n",
        "    if contains_penalty_table else \"\"\n",
        ")\n",
        "combined_context = \"\\n\\n\".join(context_blocks) + penalty_flag_note\n",
        "\n",
        "# Step 11: Build prompt\n",
        "system_prompt = \"\"\"You are an expert pension plan assistant.\n",
        "\n",
        "Use only the provided context to answer questions. Do not guess or make assumptions.\n",
        "\n",
        "If the context includes a penalty table, apply it based on the retirement age.\n",
        "Only say \"no penalty\" if the table or context explicitly says so.\n",
        "\"\"\"\n",
        "\n",
        "prompt_v10 = f\"\"\"{system_prompt}\n",
        "\n",
        "Based on the documentation below, answer the question in clear, concise terms.\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"\n",
        "{combined_context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"\\nüìù Final Prompt Sent to LLM:\\n\", prompt_v10[:1000])\n",
        "\n",
        "# Step 12: Run model\n",
        "response = llm.complete(prompt_v10)\n",
        "print(\"\\nüí¨ LLM Response:\\n\", response)\n",
        "\n",
        "# Step 13: Parse model output into variables\n",
        "parsed_vars = parse_llm_response(response.text, inputs)\n",
        "\n",
        "# Step 14: Calculate and show final pension result\n",
        "calculate_final_pension(parsed_vars)"
      ],
      "metadata": {
        "id": "RYVGV2BpHHQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}